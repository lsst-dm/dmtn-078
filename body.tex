
\section{Introduction}
In this brief note we wish to discuss some potential routes to collaboration with Google. We will revisit some of the potential ways forward to the cloud from \citeds{DMTN-072} and try to quantify what a Proof of Concept might look like. There is also scope for technical collaboration on infrastructure.

If we go ahead with one specific POC we should define more clearly the goal and duration of it and how that might feed into future operations.

{\color{red} Margaret/Michelle - should we have - expected results/metrics could also be the next round and can you look at th technical collaboration section. }
\section{Technical collaboration}
We are already heavy users of Kubernetes both internally and via Google cloud.  One assumes setting up of private clouds is something enterprises may want to do and we are pushing the envelope on that. We have specific technical issues running on our own K8S enabled hardware such as start up times\footnote{Solved for now I believe.} and security of the GPFS filesystem through K8S. Obviously help would be appreciated in this area and we may provide a large test bed outside Google for feedback on this.

\section{Cloud proof of concept}\label{sect:pocs}

Much of LSST Data Management (DM) is  Kubernetes deployable - all items mentioned here are.
This provides us with a lot of flexibility to port our system across service offerings, and would enable us to easily adopt a hybrid cloud plus on-premises infrastructure.

Moving to a cloud-based infrastructure could potentially  save on personnel, as no hands-on hardware maintenance would be required.
Although this is equivalent to a relatively small fraction of the construction budget, it would represent a substantial sum dedicated to non-core-business during operations.

We can probably not move wholescale to the cloud:
we are committed to providing the Chilean DAC in Chile, and some physical hardware must remain on the mountain and in the Commissioning Cluster.
However, there are potentially a number of opportunities to migrate a subset of DM services to the cloud if we could see a sensible way forward.

\subsection{Qserv - LSST in house database from SLAC}\label{sect:qserv}
The Qserv database system \citedsp{LDM-135} has not yet been tested in a cloud based environment.
However it is now deployable with Kubernetes, and no longer requires special hardware: physically attached storage is needed, but this is available on cloud offerings.

This is one major component of the Science Platform (\secref{sect:platform}).
Proper testing would be needed to understand how Qserv performs in the cloud environment. As a proof of concept this would be interesting: well-bounded and well-understood in terms of performance. Without the full science platform it may not be very useful in the long run beyond that.

\subsubsection{Potential needs} \label{sect:qservneeds}
{\color{red} Fritz }
To set up Qserv  we would need at least 40 large nodes with physically attached storage in to order of 2 terabytes per node.
To run a set of convincing tests we would need that up for order two months.

This would be a  demonstration only - the final catalogs (2032) will be order 15PB and the number of nodes and attached storage eventually have to scale to that size.

\subsection{Cloud based Science Platform}
\label{sect:platform}

The Science Platform \citeds{LSE-319} is intrinsically a cloud-oriented solution to the data transfer problem: it envisions user code being collocated with the data on which it is running.

The prototype DAC (PDAC) is deployed in the NCSA data facility with potential access to 2 PB of storage. There are 3 aspects: a visualization  Portal, the Jupyter Notebooks, and Web services.

The web services are an interface to the images on disk as well as the Qserv system (\secref{sect:qserv}).

A key benefit of a cloud-based Science Platform would be scalability: when user demands exceed the 10\% of the compute budget dedicated to serving them more capacity would at least be available even if it had to be purchased on demand.
There is no analogue to this in terms of on-premises infrastructure as cloud bursting from our internal cloud infrastructure to a commercial provider would require transferring potentially large amounts of data.

\subsubsection {Aside:Public Data Releases}
LSST data becomes public after 2 years. However there is no budget allocated to serve this public data - one could envision a public version of the Science Platform serving the old data as something potentially interesting for some foundations/companies e.g. to enable science in underdeveloped countries.

\subsubsection {Potential needs}
All the science platform components are deployable with Kubernetes.   The Qserv database component is a fixed size resource as discussed in \secref{sect:qservneeds}.
In addition one or preferably two servers should be provisioned for the  web services.

Alongside that one needs to have the JupyterHub environment \footnote{see https://github.com/lsst-sqre/jupyterlabdemo}; depending on the assumed load, this is relatively modest as it requires only $\sim2$ servers to set up, and it is recommended to have 2 CPUs per simultaneous user. For a proof of concept let's assume we would go with 20 simultaneous users to 40 CPUs or 10 nodes depending on the type of node. Each user should also have around 4GB of RAM.
Theoretically we would also have a batch system and more near the data compute resources but perhaps for a proof of concept this may be treated as a desirable.

{\color{red} Xiuqin }
Firefly also requires at least a pair of  server - theses should be 32 cores
with 128 GB memory. In addition these should have a shared disk volume order 500GBi, preferably SSD.

Finally there is a filesystem to store the image data. Our current code assumes a Posix filesystem, but we have made some modifications towards supporting a back-end object store. Additional investment in this direction is unlikely to happen before Fall 2018. LSST will produce over its lifetime around 60 PB of raw image data,  the final data volume including the processed images is estimated to be around 0.5 Exabytes.  For the POC 1 PB would be sufficient to see some performance and management of a large disk volume.

For the proof of concept we could leave out the Prompt Database (this is a conventional e.g. Oracle database).

\subsection{Cloud based prompt processing}\label{sect:pp}

\textit{Prompt processing} \citedsp{LDM-151} is the umbrella term used to describe processing which produces data products continuously while LSST operates.
Broadly, this falls into three categories:

\begin{enumerate}

\item{\textit{Image reduction and differencing}, in which images are received from the camera, calibrated, and compared to deep template images of the same part of sky to identify transient and variable sources;}
\item{\textit{Alert distribution}, in which notifications of transients and variables are distributed to the community;}
\item{\textit{Moving objects processing}, in which solar system objects are identified and their orbits tracked.}

\end{enumerate}

We expect to issue around $10^7$ alerts per night during normal operations.
Further, the project is required to make these alerts available to the community within no more than 60\,s of the telescope shutter closing.
This imposes stringent latency and throughput requirements on items 1 \& 2 above.
Moving object processing can be run during the day, and is therefore a less challenging — and for this purpose less interesting — use case.

One night of LSST observing generates appoximately 20\,TB of data in a ``bursty'' fashion (we visit each field for a total of 37\,s, taking two consecutive exposures which are combined by the data processing system).
In order to meet our latency requirements, we have invested in fast networking to enable rapid transfer of this data to processing systems at NCSA.

Once on the compute systems, data from each of the 189 CCDs in the camera is processed in parallel: broadly, we expect a single CPU with access to 4\,GB RAM to handle each CCD independently, taking somewhat less than a minute to complete.
In operations, we anticipate deploying two separate clusters, or ``chains'', with each processing alternating visits.

These processing chains will then feed the results of their processing to the alert distirbution system, based on Apache Kafka\footnote{\url{http://kafka.apache.org}}, for distribution to the community.
A prototype of this system has already been deployed on Amazon AWS \citeds{DMTN-028}.

This suggests two, related, proof of concept exercises:

\begin{itemize}

\item{Demonstrate rapid ingest and image processing;}
\item{Demonstrate at-scale alert distribution.}

\end{itemize}

\subsubsection{Potential needs}

Demonstrating image processing at scale could be achieved with a single processing chain (ie, 189 CPU cores, with access to 4\,GB RAM per core).
Around 60\,TB disk storage is required for a single night of data (including processed data products).
However, a smaller dataset could be defined for testing purposes.

Prompt image processing would also require a database instance to serve as the Prompt Products Database (PPDB).

Deploying a realistic alert distribution system would require three systems, each with access to 24 CPU cores and 80\,GB RAM.

\section{Conclusion}
A number of potential POCs are discussed above with approximates sizes/needs. We should pick one or more to develop further.
